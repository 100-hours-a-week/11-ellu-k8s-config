apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-server
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: vllm-server
          resources:
            requests:
              memory: "2Gi"
              cpu: "1000m"
            limits:       
              memory: "9Gi"
              cpu: "2000m"
          env:
            - name: OTEL_SERVICE_NAME
              value: "looper-vllm-server"
            - name: OTEL_SERVICE_VERSION
              value: "1.0.0"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "http://monitoring-stack-opentelemetry-collector.monitoring.svc.cluster.local:4317"
            - name: OTEL_EXPORTER_OTLP_PROTOCOL
              value: "grpc"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: "service.name=looper-vllm-server,service.version=1.0.0,service.namespace=looper,deployment.environment=prod"
            - name: OTEL_TRACES_EXPORTER
              value: "otlp"
            - name: OTLP_TRACES_ENDPOINT
              value: "http://monitoring-stack-opentelemetry-collector.monitoring.svc.cluster.local:4317"
          args:
            - |
              python3 -m vllm.entrypoints.openai.api_server \
                --model ${MODEL_NAME} \
                --host ${HOST} \
                --port ${PORT} \
                --tensor-parallel-size ${TENSOR_PARALLEL_SIZE} \
                --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION} \
                --max-model-len ${MAX_MODEL_LEN} \
                --otlp-traces-endpoint ${OTLP_TRACES_ENDPOINT}
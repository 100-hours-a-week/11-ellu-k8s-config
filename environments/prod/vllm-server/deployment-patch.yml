apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-server
spec:
  replicas: 1
  template:
    spec:
      initContainers:
        - name: install-opentelemetry
          image: python:3.12-slim
          command: ['sh', '-c']
          args:
            - |
              pip install --target=/opt/opentelemetry \
                'opentelemetry-sdk>=1.26.0,<1.27.0' \
                'opentelemetry-api>=1.26.0,<1.27.0' \
                'opentelemetry-exporter-otlp>=1.26.0,<1.27.0' \
                'opentelemetry-semantic-conventions-ai>=0.4.1,<0.5.0' \
                'opentelemetry-instrumentation-fastapi' \
                'opentelemetry-instrumentation-requests'
          volumeMounts:
            - name: opentelemetry-libs
              mountPath: /opt/opentelemetry
      containers:
        - name: vllm-server
          resources:
            requests:
              memory: "2Gi"
              cpu: "1000m"
            limits:       
              memory: "9Gi"
              cpu: "2000m"
          env:
            - name: OTEL_SERVICE_NAME
              value: "looper-vllm-server"
            - name: OTEL_SERVICE_VERSION
              value: "1.0.0"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "http://monitoring-stack-opentelemetry-collector.monitoring.svc.cluster.local:4317"
            - name: OTEL_EXPORTER_OTLP_PROTOCOL
              value: "grpc"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: "service.name=looper-vllm-server,service.version=1.0.0,service.namespace=looper,deployment.environment=prod"
            - name: OTEL_TRACES_EXPORTER
              value: "otlp"
            - name: OTLP_TRACES_ENDPOINT
              value: "http://monitoring-stack-opentelemetry-collector.monitoring.svc.cluster.local:4317"
            - name: PYTHONPATH
              value: "/opt/opentelemetry:$PYTHONPATH"
          volumeMounts:
            - name: opentelemetry-libs
              mountPath: /opt/opentelemetry
      volumes:
        - name: opentelemetry-libs
          emptyDir: {}
          args:
            - |
              python3 -m vllm.entrypoints.openai.api_server \
                --model ${MODEL_NAME} \
                --host ${HOST} \
                --port ${PORT} \
                --tensor-parallel-size ${TENSOR_PARALLEL_SIZE} \
                --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION} \
                --max-model-len ${MAX_MODEL_LEN} \
                --otlp-traces-endpoint ${OTLP_TRACES_ENDPOINT}